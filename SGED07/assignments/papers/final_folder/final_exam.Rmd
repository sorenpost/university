---
title: "SGED07: Final exam"
author: SÃ¸ren Post
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    number_sections: true
    includes:
      in_header: "preamble.tex"
bibliography:  /home/post/university/collection.bib
biblio-style: apalike
link-citations: yes
fontsize: 12pt
linestretch: 1
toc-depth: 2
secnumdepth: 2
lof: True
lot: True
---

\newpage

# Economic complexity
TODO SKRIV OM ECON COMPL (PRODUCT COMP) SOM LEGO MODELLEN
Why be interested in how complex product Indian plants produce? A recent literature on economic capabilities 

xxxx quantifies the sophistication of the productive structure in an economy by exploiting the information on the number of products it exports and the ubiquity of those products (that is, how many countries export them). 

Continuing the LEGO analogy, the product complexity values tells us about the value of the pieces present in the economy. Understanding why an economy has the landscape of LEGOs it possess is an important taks in understanding the opportunities and constraints on its economic development. 



# IN METHODS
The central idea behind the economic complexity approach is that to perform a given economic activity - like produce a product - one must possess a certain number of economic capabilities. 


As such, we can think of the relationship between countries, capabilities, and products as being connected in a tripartite network. We can only observe the countries and the products. While the best specific algorithm to extract information is still under debate, the approach is common. Essentially, two assumptions are made. First, countries possess the capabilities of the products they export and if countries export more complex products, they possess more sophisticated capabilities. Second, products that tend to be exported by more sophisticated countries require more capabilities and are thus more complex.

If we return to the LEGO analogy, this has an important implication. If more sophisticated economies possess more capabilities, they should be able to export more products in general including /both/ high- and low-complexity products. Low-sophistication economies, on the other hand, should only be able export less complex products. This fact is illustrated in figure [TODO xxx]. This means that there is more information the fact that low-sophistication economy exports a product than if a high sophistication does (given that they should be able to export most products). 

I follow the fitness-complexity algorithm developed in tachella... The algorithm follows an iterative process, where each iteration "refines" the information from the earlier by weighting country-values ("fitness") by the complexity of their products and product values by the fitness of the countries that export them. 


# Introduction and background


The first section 

Section 2 develops the conceptual framework and presents a simple model of production shocks. 

Section 3 outlines the data and the empirical strategy.



\newpage

# Conceptual framework
## O-rings and contagious disruptions
Key papers [@kremer_o-ring_1993] [@brummitt_contagious_2017] [@carvalho_micro_2014] 

Important points:
 - Shocks cascade and multiply through I/O relationships
 - More complex production processes are more vulnerable to disruptions
 - If agents optimise their production strategies, the potential for more disruptions could lead to self-selecting into less complex activities

This section develops the conceptual link between disruptions and economic complexity. First I explore the idea that disruptions in an economy can lead to producers self-selecting into less sophisticated products. Second, I discuss how the real world organization of the input-output network ... Finally, I outline how many of the simplifications in the model needs to be empirically substantiated.

This is conceptually related to Kremer's O-ring technologies...

 @brummitt_contagious_2017

## Contagious disruptions in a production network

In an model economy, all firms can be either functional or dysfunctional.

Each firm produces a single product. In order to produce this product, a firm needs a number of inputs. If the firm can source the required number of inputs, it is functional and produces a product that can be used by other firms as input. If it does not receive the required number of inputs, it becomes dysfunctional and it does not produce an output.

At any given time $t$ the share of functional firms in the economy is $F(t)$. The share of dysfunctional firms is $1 - F(t)$. A firm tries to get $m$ inputs and needs at least $\gamma$ to be successfully delivered to be functional. $m$ is then the "attempted inputs" and can be thought of as a firms in-degree in an input-output network. $\gamma$ is the threshold level required for a successful production. 

The evolving input-output network (and the disruptions) can thus be expressed through the differential equation for the expected share of functional firms at a given time:

$$
\frac{dF}{dt} = P[\text{Binomial}(m, F(t)) \geq \gamma - F(t)
$$

for $t \geq 0$ and integers $m \geq 0$ and $\gamma > 0$. When $\gamma = 0$ no firms require any inputs to their production and the model is nonsensical no firms require any inputs to their production and the model is nonsensical.

For simplicity, in each production attempt, inputs are chosen randomly (uniformly) with replacement from the population of firms. The idea is that firms do different tasks, requiring different inputs: an engineer fixes a machine on Monday, leads a meeting on Wednesday, etc (assumption is relaxed later).

All firms use the same values of $m$ and $\gamma$. Therefor the probability of a successful production is that a binomial random variable with parameters $m$ (tries) and $F(t)$ (probability of success) is larger or equal to $\gamma$ (number of critical inputs necessary for success). This means that firms can have buffers. That is, the amount $m$ is greater than $\gamma$ is the "redundancy".

Some disruptions are exogenous. Fires, insect outbreaks, weather, etc. These effects are introduced in the model as $\epsilon$. Each firm is disrupted individually and becomes dysfunctional a small rate (Poisson process). This introduces $- \epsilon F(t)$ to the differential equation as the number of functional firms that are disrupted by $\epsilon$ at time $t$:

$$
\frac{dF}{dt} = [1 - F(t)]P - F(t)(1 - P + \epsilon) 
              = P - F(t)(1 + \epsilon)
$$


Where P is the probability that a firm successfully produces its output, i.e. $P[\text{Binomial}(m, F(t)) \geq \gamma]$.

```{R functional_agent_model, echo = FALSE, messages = FALSE, fig.cap = "Test caption"}

library(tidyverse)
library(ggraph)
library(igraph)
library(wesanderson)

model_gr <- graph_from_literal(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 2 -+ 6, 4 -+ 6, 8 -+ 6,  9 -+ 6)
model_gr <- set_vertex_attr(model_gr, "fun", index = V(model_gr), c(0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1))

ggraph(model_gr, layout = 'linear') +
  geom_edge_arc(arrow = arrow(length = unit(3, 'mm')),
                start_cap = circle(5, 'mm'),
                end_cap = circle(5, 'mm'),
                strength = 0.5, # bend of curve
                fold = FALSE) + 
  coord_fixed() +
  geom_node_point(size = 10, aes(col = as.factor(fun))) +
  geom_node_text(aes(label = name)) +
  theme_graph() +
  scale_color_manual(values = wes_palette("Darjeeling2")) +
  theme(legend.position='none')

```

We can de-compose the equation above into separate terms. The first term, $1 - F(t)$ represents the rate at which dysfunctional firms attempts to produce. For each attempt, there is P probability of success. If the attempt is success, F(t) rises, if not, F(t) stays the same. The second term, $F(t)(1 - P + \epsilon)$ is the reverse. F(t) is the number of functional firms that attempt to produce. 1 - P + error is the 1 minus the probability of success, that is, the probability of dysfunction, plus til exogenous risk of dysfunction. The first term is then hos many dysfunctional firms become functional, the second hos many functional firms become dysfunctional.

The starting conditions, the amount of dysfunction 1 - F(0) is exogenous. Rest of dysfunction is entirely endogenous. All of this dysfunction is driven by the effect that a firm can only deliver the input to other firms if it is successful it the most recent attempt to produce.

Choosing complexity ($\gamma$) and buffers ($m - \gamma$): To include the incentive to produce more complex products, a utility function is added, where firms derive utility from producing more goods that require more inputs. This utility is expressed by $\gamma^{\beta}$ where $\beta$ is between 0 and 1. It also assumed that each attempted input has a cost of $\alpha$, wehere $\alpha$ is more than 0. This is the marginal cost of finding a supplier, arranging input, etc. Again for simplicity, it is assumed that each firm knows thew likelihood that a supplier will successfully produce and deliver the required input, F(t). On the basis of this reliability, a form can revise its strategy on how complex the product should be: $\gamma \in \{0, 1, 2, ...\}$, as well as how many inputs should be attempted: $m \in \{0, 1, 2, ...\}$.

Each firm must commit to a product for a certain amount of time $T$, so decisions are updated after every $T$ amount of time (at the same time). They change their behaviour to the "best response", the maximisers  $m*, \gamma*$ of the utility function

$$
U [m, \gamma, F(t); \alpha, \beta] = P[m, \gamma, F(t)]\gamma^{\beta} - \alpha m
$$

It is then define


\newpage
## Propagation of shocks in a static I/O network
# A simple model of disruptions

Say that in a simplified economy, production happens at $n$ distinct nodes. 

Lets assume that in this simple model, each node's production resembles a Cobb-Douglas function with constant returns to scale. Inside each production, a primary good (labor) is combined with some combination of intermediate inputs. The output of each node can then be sold to households or be used for other nodes input in their production. We can then describe the output $x$ of a given sector $i$ as 

$$
x_i = (z_il_i)^{1-\alpha} (\prod^{n}_{i = 1}x^{w_{ij}}_{ij})^{\alpha}
$$

Here, the first term shows the contribution from the amount of labour hired ($l_i$) and its share in production ($1 - \alpha$). $z_{ij}$ represents exogenous shocks to the production. These shocks are independent across different nodes in the economy, and is the only source of randomness in this model. 

The second term, $(\prod^{n}_{j = 1} x^{w_{ij}}_{ij})^{\alpha}$, represents the connection between different nodes in the economy. $x_{ij}$ is the amount of good $j$ (that is, the output from node $j$) used in the production of good $i$. $w_{ij}$ gives a kind of production recipe for $x_i$: it gives the share of good $j$ in the total use of intermediate inputs in $i$. If $w_{ij}$ is zero, product $j$ is not used in the production inputs and the term equals 1. Then, any changes in the availability of $j$ does not impact the output of the node $i$. 

An implicit assumption in this model is that each intermediary input is complementary and non-substituable. This is a sort of "O-ring" technology^[In the sense of ] of inputs, where production requires a specific combination of necessary inputs ($w_{ij} > 0$). If one of these inputs is missing the rest of the inputs are useless. [kan sagtens kombineres med Brummit-modellen - man kan godt "source" fra flere firmaer]. 




## Something dumb
```{R i_o_rel_model, include = FALSE, echo = FALSE, warnings = FALSE, messages = FALSE, fig.cap = "Test caption 2"}
library(tidyverse)
library(ggraph)
library(igraph)
library(wesanderson)
library(ggpubr)

node_size <- 10
node_color <- "#046C9A"
text_color <- "#000000"


iso_g <- graph_from_literal(1, 2, 3, 4)

iso_p <- ggraph(iso_g, layout = 'linear') +
  geom_edge_arc(arrow = arrow(length = unit(3, 'mm')),
                start_cap = circle(5, 'mm'),
                end_cap = circle(5, 'mm')) +
  geom_node_point(size = node_size, color = node_color) +
  geom_node_text(aes(label = name), color = text_color) +
  theme_graph() +
  theme(legend.position='none')

vert_g <- graph_from_literal(1 -+ 2, 2 -+ 3, 3 -+ 4)

vert_p <- ggraph(vert_g) +
  geom_edge_link(arrow = arrow(length = unit(3, 'mm')),
                start_cap = circle(5, 'mm'),
                end_cap = circle(5, 'mm')) +
  geom_node_point(size = node_size, color = node_color) +
  geom_node_text(aes(label = name), color = text_color) +
  theme_graph() +
  theme(legend.position='none')

hub_g <- graph_from_literal(1 ++ 2, 1 ++ 3, 1 ++ 4)

hub_p <- ggraph(hub_g, layout = 'graphopt') +
  geom_edge_fan(arrow = arrow(length = unit(3, 'mm')),
                start_cap = circle(5, 'mm'),
                end_cap = circle(5, 'mm')) +
  geom_node_point(size = node_size, color = node_color) +
  geom_node_text(aes(label = name), color = text_color) +
  theme_graph() +
  theme(legend.position='none')

figure <- ggarrange(iso_p, vert_p, hub_p,
                    labels = c("A", "B", "C"),
                    ncol = 3, nrow = 1)

figure
```

!["Test"](/home/post/university/SGED10/SGED07 final/three_models_io.png)




w er god fordi outputs der ikke bruges hedder x^0 = 1




# Conceptual framework

\newpage
  
# Research questions

## Research aims

## Questions

# Methodology
This section presents the general research design and the construction of the key variables used in the regression analysis. First I outline the empirical strategy ... 

The main feature of the study is designed to exploit the state-level variation in electricity shortages to explain differences in the complexity of products produced by individual plants. At the most general level, this includes three steps. 
First, I construct a product-complexity index and assign a value to each plants production. Next I estimate average electricity shortages for each state. This state-level value is also assigned to each plant. Finally, I regress the state-level electricity-variable on the plant-level complexity variable and a number of relevant controls.

## Empirical strategy

### Product Complexity
The central idea behind the economic complexity approach is that to perform a given economic activity - like produce a product - one must possess a certain number of economic capabilities. 


As such, we can think of the relationship between countries, capabilities, and products as being connected in a tripartite network. We can only observe the countries and the products. While the best specific algorithm to extract information is still under debate, the approach is common. Essentially, two assumptions are made. First, countries possess the capabilities of the products they export and if countries export more complex products, they possess more sophisticated capabilities. Second, products that tend to be exported by more sophisticated countries require more capabilities and are thus more complex.

If we return to the LEGO analogy, this has an important implication. If more sophisticated economies possess more capabilities, they should be able to export more products in general including /both/ high- and low-complexity products. Low-sophistication economies, on the other hand, should only be able export less complex products. This fact is illustrated in figure [TODO xxx]. This means that there is more information the fact that low-sophistication economy exports a product than if a high sophistication does (given that they should be able to export most products). 

I follow the fitness-complexity algorithm developed in tachella... The algorithm follows an iterative process, where each iteration "refines" the information from the earlier by weighting country-values ("fitness") by the complexity of their products and product values by the fitness of the countries that export them. 

I calculate product complexity-values based on the Method of Reflections developed in TODO. 

- RCA => Mcp

- Formular

- Graph

### Plant complexity
For each plant, I quantify the complexity of their production output as the weighted average the complexity-values for each product it produces. I assign weights based on the value of the production. That is, the complexity for factory $f$ at time $t$, $C_{ft}$, is defined as:

$$
C_{ft} = \sum_p Q_{pt} \frac{O_{fpt}}{\sum_p O_{fpt}}
$$

where $Q_{pt}$ is the product complexity of product $p$ at time $t$ and $O_{fpt}$ is the output (in current prices) of factory $f$ in product $p$ at time $t$. 

However, this definition potentially underestimates the complexity of large, multi-product factories that produce very complex products, but happen to sell markedly more of their low-complex ones. I therefor also include a stricter measure of plant complexity ($C^{\text{max}}_{ft}$) that uses only the most complex product in a factory's product-portfolio, regardless of the output volume. 

$$
C^{\text{max}}_{ft} = max \{ Q_{1t} I_{1ft}, \text{ ...}, Q_{pt} I_{pt} \}
$$

where 

$$
I_{pft} = \begin{cases}
 1 & \text{if } O_{fpt} \geq 0 \\ 
 0 & \text{if } O_{fpt} = 0
\end{cases}
$$

### Electricity shortages
TODO

### Instrumental variable

There are a couple of reasons the effect of electricity disruptions on economic activities are diffult to study empirically. First, the relationship is likely to have a significant endogenous component. More complex production could be related to a more intensely developed economy, which could also be related to mre stabile electricity supply. On the other hand, a more developed economy could have a more complex production, but would also have a higher electricity demand which could lead to shortages. Secondly, both the measurements of electricity used in this study very likely suffers from measurement error, leading to attenuation bias. Ã¦ 

I therefor follow @allcott_how_2016 in constructing an instrumental variable based on hydeo-electricity generation. A good instrument must affect the supply of electricity, but impact only the manufactoring plants through shortages. Hydro plants have a very low marginal cost of generating electricity, and the yearly generation of electricity is therefor primarily dependent on water availbility. This water availability is determined by rain- and snowfall at higher elevations.

The instrumental variable $Z_{st}$ is defined as the share of the predicted hydro plant generation $H_{st}$ in the total predicted electricity demand $\hat{Q}_{st}$:

$$
Z_{st} = \frac{H_{st}}{\hat{Q}_{st}}
$$

Shortages of electricity directly impacts the consumption of electricity. Therefor, the predicted electricity for a state $s$ is the average electricity of the other states (without state $s$) times the average ratio of state $s$ electricity use as a share of the given year's total electricity use in the other states (again not including $s$). This means that the predicted electricity use for state $s$ in year $t$, $\hat{Q}_{st}$ is defined as:

$$
\hat{Q}_{st} = \frac{1}{R-1} \sum_{r \neq s}^{R-1}Q_{rt} \cdot \frac{1}{Y} \sum_{y}^{Y} \frac{Q_{sy}}{\sum_{r \neq s}^{R-1} Q_{ry}}
$$

where states are indexed by $r$, $R$ is the total number of states, years are indexed by $y$, and $Y$ is the total number of years in the sample.

## Estimation

## Limitations



\newpage



## Electricity and shortages
[Skriv en del af dette om til formatet: My main data on ... comes from Allcott et al. The dataset contains ... from ...]

I use several distinct types of data on electricity disruptions. The first is a measure of shortages based on the difference between the observed consumption of electricity and the estimated counterfactual demand. This

Electricity shortages are measured as the percent energy deficit listed in the Load Generation Balance Reports [@cea_load_1993] released by India's Central Electricity Authority (CEA). In conjunction with the CEA, @allcott_how_2016 collected, cleaned and digitized the electricity data stretching back to 1992^[The data is provided by the authours at www.indiaenergydata.info.].

At the end of each year, the CEA and the Regional Power Committees estimate the monthly counterfactual quantity that would have been demanded in each state if there were no shortages. This annual figure, listed in current prices, is the *assessed demand* ($A$). The sum of electricity available from power plants and net imports is the *energy available* ($E$). The measure of shortages ($S$) is then defined by the CEA as the percent of demand in state $s$ in year $t$ that is not met:

$$
S_{st} = \frac{A_{st} - E_{st} }{A_{st}}
$$

In addition, the CEA reports a measure of the power shortages during peak hours ($S^p$). This "peak shortage" is defined analogously to $S$ but using only *peak assessed demand* ($A^{p}$) and *peak energy available* ($E^p$):

$$
S^{p}_{st} = \frac{A^{p}_{st} - E^{p}_{st}}{A^{p}_{st}}
$$

The data on total electricity sold to consumers, the total hydroelectrict electriction generation (both observed and maximum capacity), the total generation capacity, and the previous yeraas capacity increase (state/year) comes from the General Review [@cea_all_1994] and provided by 


## Nightlight estimation of shortages

## Annual Survey of Industries (ASI)
I use 

## Weather data

## International trade data
To construct the index of product complexity, I use data on international trade.
The raw trade data is collected by UN COMTRADE and provided through the Harvard Growth Lab [@the_growth_lab_at_harvard_university_international_2019]. I use the Harmonized System 1992 (HS92) classification, which covers bilateral trade flows for approximately 5000 products between 1995 and 2017. 

The bilateral flows are aggregated into product-level exports for each country, for each year. 

# Notes


- Limitation: Product quality difference within same product category. 

- Ideen om mere komplekse produkter er relateret til Kremer's O-ring papir sektion 1.4 om choice of technology.

# References
