---
title: "Answers: chapter 2"
author: Søren Post
date: "`r Sys.Date()`"
output: html_document
---

```{r render, eval = FALSE, echo = FALSE}
library(rmarkdown)
rmarkdown::render("~/university/notes/regression_ex.Rmd", output_dir = "~/university/")
```


# Chapter 2
## Question 1: Linear transformation of random variable.

A test is graded from 0 to 50, with an average score of 35 and a standard deviation of 10. For comparison to other tests, it would be convenient to rescale to a mean of 100 and standard deviation of 15.

2: *How can the scores be linearly transformed to have this new mean and standard deviation?*

Vi ved at hvis X er en random variable, og b og a er konstanter, så er a + bX en linær transformation der skubber X med a og skalere X med b. Denne nye variable noteres typisk Z.

E[Z] = E[a + bX] = a + b * E[X]

$\sigma^2_z$ = Var[a + bX] = b^2 $\sigma^2_x$

Hvis vi har E[Z] = 100 og $\sigma^2_z$ = 15^2, så kan vi altså solve til både a og b.

E[Z] = a + bE[X]
100 = a + b35

og

$\sigma^2_z$ = Var[a + bX] = b^2 $\sigma^2_x$
15^2 = Var[a + bX] = b^2 10^2

Først solver vi for b:
10^2b^2 = 15^2
 b = 1.5

Så solver vi for a:
100 = a + (1.5 * 35)
a = 47.5

2: *There is another linear transformation that also rescales the scores to have mean 100 and standard deviation 15. What is it, and why would you not want to use it for this purpose?*

Når vi solver for b, så kan vi også få -1.5 som det rigtige svar. Det betyder at den linær transformation istedet bliver: Z = -1.5x + 152.5.

Dette er ikke så smart, fordi det ødelægger vores transformations egenskaber hvsi b er negativ. For eksempel: i den originale variable, så har en elen en score på 35 (mean) og en elev en score på 25 (en sd under mean). Men i  vores nye transformation:

Z(25) = -1.5 * 25 + 152.5 = 115

Z(35) = -1.5 * 35 + 152.5 = 100

er den lavere score nu højere. Dette er noget lort.

## TODO Question 2: Proportions and standard deviation

The following are the proportions of girl births in Vienna for each month in 1908 and 1909 (out of an average of 3900 births per month):

.4777 .4875 .4859 .4754 .4874 .4864 .4813 .4787 .4895 .4797 .4876 .4859
.4857 .4907 .5010 .4903 .4860 .4911 .4871 .4725 .4822 .4870 .4823 .4973
    
The data are in the folder girls. von Mises (1957) used these proportions to claim that the sex ratios were less variable than would be expected by chance.

*1: Compute the standard deviation of these proportions and compare to the standard deviation that would be expected if the sexes of babies were independently decided with a constant probability over the 24-month period.*

Antallet af piger der er født per måned kan modelleres som en Poisson process.

```{R} 
library(tidyverse)

girls <- c(.4777, .4875, .4859, .4754, .4874, .4864, .4813, .4787, .4895, .4797, .4876, .4859, .4857, .4907, .5010, .4903, .4860, .4911, .4871, .4725, .4822, .4870, .4823, .4973)
```

*2: The actual and theoretical standard deviations from (a) differ, of course. Is this difference statistically significant? (Hint: under the randomness model, the actual variance should have a distribution with expected value equal to the theoretical variance, and proportional to a χ2 with 23 degrees of freedom.)**

## Question 3: Demonstration of the Central Limit Theorem:

let $X = x_1 + ··· + x_{20}$, the sum of 20 independent Uniform(0,1) random variables. In R, create 1000 simulations of x and plot their histogram. On the histogram, overlay a graph of the normal density function. Comment on any differences between the histogram and the curve.

```{R}

library(tidyverse)

distribution <- vector()

# 1000 draws
for (i in 1:1000) {
X <- sum(
  runif(n = 20, 0, 1)
  )

distribution[i] <- X
}

dist_df <- tibble(distribution)


ggplot(data = dist_df, aes(x = distribution)) +
  geom_histogram(mapping = aes(y = stat(density))) +
  stat_function(fun = dnorm,
                 args = list(mean = mean(dist_df$distribution), sd = sd(dist_df$distribution)),
                 color = 'red')

# 100 000 draws
for (i in 1:100000) {
X <- sum(
  runif(n = 20, 0, 1)
  )

distribution[i] <- X
}

dist_df <- tibble(distribution)


ggplot(data = dist_df, aes(x = distribution)) +
  geom_histogram(mapping = aes(y = stat(density))) +
  stat_function(fun = dnorm,
                 args = list(mean = mean(dist_df$distribution), sd = sd(dist_df$distribution)),
                 color = 'red')

```

Som foventet under central limit theorem, så er fordelingen omtrent normal fordelt. Hvis vi øger antallet af draws af X så bliver den mere normal fordelt.

## Question 4: Distribution of averages and differences:

the heights of men in the United States are approximately normally distributed with mean 69.1 inches and standard deviation 2.9 inches. The heights of women are approximately normally distributed with mean 63.7 inches and standard deviation 2.7 inches. Let x be the average height of 100 randomly sampled men, and y be the average height of 100 randomly sampled women. In R, create 1000 simulations of x − y and plot their histogram. Using the simulations, compute the mean and standard deviation of the distribution of x − y and compare to their exact values.

```{R}

diff_v <- vector()

for(i in 1:1000) {
men <- rnorm(n = 100, mean = 69.1, sd = 2.9)
women <- rnorm(n = 100, mean = 63.7, sd = 2.7)

x <- mean(men)
y <- mean(women)

diff_v[i] <- x - y

}

diff_df <- tibble(diff_v)

ggplot(diff_df, aes(x = diff_v)) +
  geom_histogram()

sim_mean <- mean(diff_v)
sim_sd <- sd(diff_v)
obs_mean <- 69.1 - 63.7
obs_sd <- sqrt((2.92)^2/100 + (2.72)^2/100)


sim_mean / obs_mean
sim_sd / obs_sd

```

Den simulerede mean og SD er meget tæt på den observerede.

## Question 5: random variables:
suppose that the heights of husbands and wives have a correlation of 0.3. Let x and y be the heights of a married couple chosen at random. What are the mean and standard deviation of the average height, (x + y)/2?

Hvis x og y er random variables med means $\mu_x$, $\mu_y$ og correlation $\rho$ så kan vi finde deres fælles mean ved $a \mu_x + b \mu_y$ hvor a = b = 0.5. Deres fælles standard deviation er derved $\sqrt{a^2\sigma_x^2 + b^2 \sigma_y^2 + 2 a b \rho \sigma_x \sigma_y}$.


Udregningen er derved forholdsvist simpel:

```{R}

mu_x <- 69.1
mu_y <- 63.7

sd_x <- 2.9
sd_y <- 2.7

rho <- 0.3

a <- 0.5
b <- 0.5

com_mean <- a * mu_x + b * mu_y
com_sd <- sqrt( (a^2 * sd_x^2) + (b^2 * sd_y^2) + 2 * a * b * rho * sd_y * sd_x )

com_mean
com_sd

```
